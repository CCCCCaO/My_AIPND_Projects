{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练神经网络\n",
    "\n",
    "我们在上个部分构建的神经网络了解的信息很少，它不知道关于我们的手写数字的任何信息。具有非线性激活函数的神经网络就像通用函数逼近器一样。某些函数会将输入映射到输出。例如，将手写数字图像映射到类别概率。神经网络的强大之处是我们可以训练网络以逼近这个函数，基本上只要提供充足的数据和计算时间，任何函数都可以逼近。\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "一开始网络很朴素，不知道将输入映射到输出的函数。我们通过向网络展示实际数据样本训练网络，然后调整网络参数，使其逼近此函数。\n",
    "\n",
    "要找到这些参数，我们需要了解网络预测真实输出的效果如何。为此，我们将计算**损失函数**（也称为成本），一种衡量预测错误的指标。例如，回归问题和二元分类问题经常使用均方损失\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "其中 $n$ 是训练样本的数量，$y_i$ 是真正的标签，$\\hat{y}_i$ 是预测标签。\n",
    "\n",
    "通过尽量减小相对于网络参数的这一损失，我们可以找到损失最低且网络能够以很高的准确率预测正确标签的配置。我们使用叫做**梯度下降法**的流程来寻找这一最低值。梯度是损失函数的斜率，指向变化最快的方向。要以最短的时间找到最低值，我们需要沿着梯度（向下）前进。可以将这一过程看做沿着最陡的路线下山。\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>\n",
    "\n",
    "## 反向传播\n",
    "\n",
    "对于单层网络，梯度下降法实现起来很简单。但是，对于更深、层级更多的神经网络（例如我们构建的网络），梯度下降法实现起来更复杂，以至于需要大约 30 年时间研究人员才能弄明白如何训练多层网络，虽然了解这一概念后会发现很简单。\n",
    "\n",
    "我们通过**反向传播**来实现，实际上是采用的微积分中的链式法则。最简单的理解方法是将两层网络转换为图形表示法。\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "在网络的前向传递过程中，我们的数据和运算从左到右。要通过梯度下降法训练权重，我们沿着网络反向传播成本梯度。从数学角度来讲，其实就是使用链式法则计算相对于权重的损失梯度。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "我们使用此梯度和学习速率 $\\alpha$ 更新权重。\n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "设置学习速率的方式是权重更新步长很小，使迭代方法能达到最小值。\n",
    "\n",
    "对于训练步骤来说，首先我们需要定义损失函数。在 PyTorch 中，通常你会看到它写成了 `criterion` 形式。在此例中，我们使用 softmax 输出，因此我们希望使用 `criterion = nn.CrossEntropyLoss()` 作为损失函数。稍后在训练时，你需要使用 `loss = criterion(output, targets)` 计算实际损失。\n",
    "\n",
    "我们还需要定义优化器，例如 SGD 或 Adam 等。我将使用 SGD，即 `torch.optim.SGD`，并传入网络参数和学习速率。\n",
    "\n",
    "## Autograd\n",
    "\n",
    "Torch 提供了模块 `autograd` 用于自动计算张量的梯度。计算方式是跟踪在张量上执行的运算。要让 PyTorch 跟踪运算，你需要使用 `torch.autograd` 的 `Variable` 类封装张量。你可以使用 Variable 的 `.data` 属性获取张量。\n",
    "\n",
    "我们使用 `z.backward()` 计算相对于变量 `z` 的梯度。这样会对创建 `z` 的运算进行反向传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7294, -1.1111],\n",
      "        [-1.3306,  1.7804]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个随机张量x 2*2规模 并且使用autograd跟踪发生在此张量上所有的运算\n",
    "x = torch.randn(2,2)\n",
    "x = Variable(x, requires_grad=True)\n",
    "# 上面那个也可以写成 x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5320,  1.2345],\n",
      "        [ 1.7705,  3.1698]])\n"
     ]
    }
   ],
   "source": [
    "# 对x张量进行2次幂计算\n",
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在下面看到创建 `y` 的运算，即幂运算 `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7f1683723438>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable 这里显示pow即幂次\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autgrad 模块会跟踪这些运算并知道如何为每个运算计算梯度。这样的话，它就能够计算一系列运算相对于任何一个张量的梯度。我们将张量 `y` 简化为标量值，即均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6767)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以查看 `x` 和 `y` 的梯度，但是现在它们为空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要计算梯度，你需要对 Variable（例如 `z`）运行 `.backward` 方法。这样会计算 `z` 相对于 `x` 的梯度\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3647, -0.5555],\n",
      "        [-0.6653,  0.8902]])\n",
      "tensor([[ 0.3647, -0.5555],\n",
      "        [-0.6653,  0.8902]])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些梯度运算对神经网络来说尤其有用。对于训练来说，我们需要得出权重相对于成本的梯度。对于 PyTorch，我们在网络中向前运行数据以计算成本，然后向后传播以计算相对于成本的梯度。得出梯度后，我们可以执行梯度下降步。\n",
    "\n",
    "## 训练网络！\n",
    "\n",
    "对于训练步骤来说，首先我们需要定义损失函数。在 PyTorch 中，通常你会看到它写成了 `criterion` 形式。在此例中，我们使用 softmax 输出，因此我们希望使用 `criterion = nn.CrossEntropyLoss()` 作为损失函数。稍后在训练时，你需要使用 `loss = criterion(output, targets)` 计算实际损失。\n",
    "\n",
    "我们还需要定义优化器，例如 SGD 或 Adam 等。我将使用 SGD，即 `torch.optim.SGD`，并传入网络参数和学习速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和上一章节一样 获取数据并构建神经网络\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 200, 50, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' This function for predicts classes by calculating the softmax '''\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "# 定义损失函数 使用交叉熵\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 定义优化器 使用随机梯度下降法 模型的所有参数 以及学习率\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们只考虑一个学习步，然后再循环访问所有数据。PyTorch 的一般流程是：\n",
    "\n",
    "* 在网络中进行前向传递以获得 logits\n",
    "* 使用 logits 计算损失\n",
    "* 通过 `loss.backward()` 对网络进行反向传递以计算梯度\n",
    "* 用优化器执行一步以更新权重\n",
    "\n",
    "我将在下面完成一个训练步并输出权重和梯度，使你能够明白变化过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-1.3379e-02, -7.5647e-03,  2.9398e-02,  ..., -1.4258e-02,\n",
      "          1.5749e-02,  1.8556e-02],\n",
      "        [ 1.1815e-02,  2.3139e-02, -1.1971e-02,  ..., -2.2807e-03,\n",
      "          2.1635e-02,  1.3979e-02],\n",
      "        [-8.8997e-03, -2.1532e-02,  8.7935e-04,  ...,  2.0745e-02,\n",
      "          3.0499e-04, -6.2171e-03],\n",
      "        ...,\n",
      "        [-2.7706e-03, -1.5943e-02, -1.6606e-02,  ...,  3.3453e-03,\n",
      "         -1.8223e-02,  2.6415e-02],\n",
      "        [-1.5857e-02, -1.7062e-02,  2.0083e-03,  ..., -1.5585e-02,\n",
      "          9.1716e-03,  1.8211e-03],\n",
      "        [ 2.0042e-02, -1.5072e-02,  2.6468e-02,  ..., -2.9938e-02,\n",
      "          4.6096e-03, -3.3216e-02]])\n",
      "Gradient - tensor(1.00000e-02 *\n",
      "       [[ 0.0895,  0.0895,  0.0895,  ...,  0.0895,  0.0895,  0.0895],\n",
      "        [ 0.1085,  0.1085,  0.1085,  ...,  0.1085,  0.1085,  0.1085],\n",
      "        [ 0.1019,  0.1019,  0.1019,  ...,  0.1019,  0.1019,  0.1019],\n",
      "        ...,\n",
      "        [-0.0969, -0.0969, -0.0969,  ..., -0.0969, -0.0969, -0.0969],\n",
      "        [ 0.0613,  0.0613,  0.0613,  ...,  0.0613,  0.0613,  0.0613],\n",
      "        [-0.1071, -0.1071, -0.1071,  ..., -0.1071, -0.1071, -0.1071]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', net.fc1.weight)\n",
    "\n",
    "# 这两行也可以写成 images, labels = next(iter(trainloader))噢！\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Create Variables for the inputs and targets\n",
    "inputs = Variable(images)\n",
    "targets = Variable(labels)\n",
    "\n",
    "# Clear the gradients from all Variables 防止梯度被累加 这里梯度要归零\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights 前向传播\n",
    "output = net.forward(inputs) # 输出\n",
    "loss = criterion(output, targets) # 损失\n",
    "loss.backward() # 反向传播\n",
    "print('Gradient -', net.fc1.weight.grad)\n",
    "optimizer.step() # 更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-1.3388e-02, -7.5736e-03,  2.9389e-02,  ..., -1.4267e-02,\n",
      "          1.5740e-02,  1.8547e-02],\n",
      "        [ 1.1804e-02,  2.3128e-02, -1.1982e-02,  ..., -2.2916e-03,\n",
      "          2.1624e-02,  1.3968e-02],\n",
      "        [-8.9099e-03, -2.1542e-02,  8.6917e-04,  ...,  2.0735e-02,\n",
      "          2.9481e-04, -6.2273e-03],\n",
      "        ...,\n",
      "        [-2.7609e-03, -1.5933e-02, -1.6596e-02,  ...,  3.3550e-03,\n",
      "         -1.8214e-02,  2.6425e-02],\n",
      "        [-1.5863e-02, -1.7068e-02,  2.0022e-03,  ..., -1.5592e-02,\n",
      "          9.1655e-03,  1.8150e-03],\n",
      "        [ 2.0053e-02, -1.5062e-02,  2.6478e-02,  ..., -2.9927e-02,\n",
      "          4.6203e-03, -3.3205e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', net.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实际训练\n",
    "\n",
    "现在，我们将此算法用于循环中，以便访问所有图像。很简单，我们将循环访问数据集的小批次数据，在网络中传递数据以计算损失，获得梯度，然后运行优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # 这里使用了Adam优化器！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 1.9159 Test accuracy: 0.5938\n",
      "Epoch: 1/1 Loss: 1.1698 Test accuracy: 0.7364\n",
      "Epoch: 1/1 Loss: 0.7557 Test accuracy: 0.8082\n",
      "Epoch: 1/1 Loss: 0.6133 Test accuracy: 0.8595\n",
      "Epoch: 1/1 Loss: 0.5108 Test accuracy: 0.8612\n",
      "Epoch: 1/1 Loss: 0.4333 Test accuracy: 0.8798\n",
      "Epoch: 1/1 Loss: 0.4512 Test accuracy: 0.8678\n",
      "Epoch: 1/1 Loss: 0.4061 Test accuracy: 0.8793\n",
      "Epoch: 1/1 Loss: 0.4162 Test accuracy: 0.8910\n",
      "Epoch: 1/1 Loss: 0.4115 Test accuracy: 0.8823\n",
      "Epoch: 1/1 Loss: 0.4240 Test accuracy: 0.8884\n",
      "Epoch: 1/1 Loss: 0.3833 Test accuracy: 0.8988\n",
      "Epoch: 1/1 Loss: 0.3797 Test accuracy: 0.9005\n",
      "Epoch: 1/1 Loss: 0.3872 Test accuracy: 0.9040\n",
      "Epoch: 1/1 Loss: 0.3533 Test accuracy: 0.8993\n",
      "Epoch: 1/1 Loss: 0.3327 Test accuracy: 0.8901\n",
      "Epoch: 1/1 Loss: 0.3474 Test accuracy: 0.9101\n",
      "Epoch: 1/1 Loss: 0.2998 Test accuracy: 0.9149\n",
      "Epoch: 1/1 Loss: 0.3463 Test accuracy: 0.9151\n",
      "Epoch: 1/1 Loss: 0.3190 Test accuracy: 0.9125\n",
      "Epoch: 1/1 Loss: 0.3373 Test accuracy: 0.9117\n",
      "Epoch: 1/1 Loss: 0.2735 Test accuracy: 0.9158\n",
      "Epoch: 1/1 Loss: 0.3200 Test accuracy: 0.9194\n",
      "Epoch: 1/1 Loss: 0.3597 Test accuracy: 0.9047\n",
      "Epoch: 1/1 Loss: 0.2885 Test accuracy: 0.9182\n",
      "Epoch: 1/1 Loss: 0.2637 Test accuracy: 0.9193\n",
      "Epoch: 1/1 Loss: 0.2436 Test accuracy: 0.9163\n",
      "Epoch: 1/1 Loss: 0.3030 Test accuracy: 0.9256\n",
      "Epoch: 1/1 Loss: 0.2765 Test accuracy: 0.9228\n",
      "Epoch: 1/1 Loss: 0.2619 Test accuracy: 0.9233\n",
      "Epoch: 1/1 Loss: 0.2575 Test accuracy: 0.9250\n",
      "Epoch: 1/1 Loss: 0.2707 Test accuracy: 0.9289\n",
      "Epoch: 1/1 Loss: 0.2977 Test accuracy: 0.9165\n",
      "Epoch: 1/1 Loss: 0.2683 Test accuracy: 0.9330\n",
      "Epoch: 1/1 Loss: 0.2290 Test accuracy: 0.9247\n",
      "Epoch: 1/1 Loss: 0.2356 Test accuracy: 0.9346\n",
      "Epoch: 1/1 Loss: 0.2201 Test accuracy: 0.9353\n",
      "Epoch: 1/1 Loss: 0.2187 Test accuracy: 0.9269\n",
      "Epoch: 1/1 Loss: 0.2611 Test accuracy: 0.9191\n",
      "Epoch: 1/1 Loss: 0.2439 Test accuracy: 0.9352\n",
      "Epoch: 1/1 Loss: 0.2845 Test accuracy: 0.9302\n",
      "Epoch: 1/1 Loss: 0.2365 Test accuracy: 0.9390\n",
      "Epoch: 1/1 Loss: 0.2759 Test accuracy: 0.9352\n",
      "Epoch: 1/1 Loss: 0.2156 Test accuracy: 0.9280\n",
      "Epoch: 1/1 Loss: 0.2040 Test accuracy: 0.9407\n",
      "Epoch: 1/1 Loss: 0.2085 Test accuracy: 0.9377\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 # 训练的次数 比如定3 则将整个数据集过三次\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 20\n",
    "# 将上面一次前向计算和反向传播 包装在for循环中\n",
    "for e in range(epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 这一句也可以写成running_loss += loss.item() 因为这里loss是一个标量张量\n",
    "        # 而不是简单的数字！要么用索引 要么用items方法\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        # 输出训练损失\n",
    "        if steps % print_every == 0:\n",
    "            # Test accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                \n",
    "                predicted = net.predict(inputs).data\n",
    "                equality = (labels == predicted.max(1)[1])\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}\".format(accuracy/(ii+1)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1640e88780>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = images[2]\n",
    "ps = net.predict(Variable(img.resize_(1, 784)))\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的网络现在并不是一无所知了。它可以准确地预测图像中的数字。接着，你将编写用更复杂的数据集训练神经网络的代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
