{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存和加载模型\n",
    "\n",
    "在这个 notebook 中，我将为你展示如何使用 Pytorch 来保存和加载模型。这个步骤十分重要，因为你一定希望能够加载预先训练好的模型来进行预测，或是根据新数据继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们可以看见一张图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15f070dcf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_4_0.png)\n",
    "\n",
    "\n",
    "## 构建网络\n",
    "\n",
    "在这里，我将使用与第五部分中一样的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络\n",
    "\n",
    "并使用之前一样的方法来训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "model = Network(784, 10, [500, 100])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.128..  Test Loss: 0.668..  Test Accuracy: 0.746\n",
      "Epoch: 1/2..  Training Loss: 0.730..  Test Loss: 0.606..  Test Accuracy: 0.759\n",
      "Epoch: 1/2..  Training Loss: 0.667..  Test Loss: 0.550..  Test Accuracy: 0.788\n",
      "Epoch: 1/2..  Training Loss: 0.603..  Test Loss: 0.550..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.582..  Test Loss: 0.507..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.568..  Test Loss: 0.483..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.552..  Test Loss: 0.476..  Test Accuracy: 0.824\n",
      "Epoch: 1/2..  Training Loss: 0.557..  Test Loss: 0.468..  Test Accuracy: 0.831\n",
      "Epoch: 1/2..  Training Loss: 0.544..  Test Loss: 0.474..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.466..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.460..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.446..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.487..  Test Loss: 0.453..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.504..  Test Loss: 0.447..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.486..  Test Loss: 0.437..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.507..  Test Loss: 0.434..  Test Accuracy: 0.842\n",
      "Epoch: 2/2..  Training Loss: 0.477..  Test Loss: 0.435..  Test Accuracy: 0.844\n",
      "Epoch: 2/2..  Training Loss: 0.501..  Test Loss: 0.433..  Test Accuracy: 0.844\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 100\n",
    "for e in range(epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            model.eval()\n",
    "            \n",
    "            accuracy = 0\n",
    "            test_loss = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                # Set volatile to True so we don't save the history\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                labels = Variable(labels, volatile=True)\n",
    "\n",
    "                output = model.forward(inputs)\n",
    "                test_loss += criterion(output, labels).data[0]\n",
    "                \n",
    "                ## Calculating the accuracy \n",
    "                # Model's output is log-softmax, take exponential to get the probabilities\n",
    "                ps = torch.exp(output).data\n",
    "                # Class with highest probability is our predicted class, compare with true label\n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和加载模型\n",
    "\n",
    "可以想象，在每次使用神经网络时都重新进行训练很不现实。因此，我们可以保存之前训练好的网络，并在继续训练或是进行预测时加载网络。\n",
    "\n",
    "PyTorch 网络的参数都存储在模型的 `state_dict` 中。我们可以看到这个状态字典包含了每个层的权重和偏差矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的做法是使用 `torch.save` 来保存状态字典。比如，我们可以将它保存到文件 `'checkpoint.pth'` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 网络的参数都存储在模型的 state_dict 中\n",
    "# 我们可以看到这个状态字典包含了每个层的权重和偏差矩阵\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们可以使用 `torch.load` 来加载这个状态字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将状态字典加载到神经网络中，你需要使用 `model.load_state_dict(state_dict)'`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这看上去十分简单，但实际情况更加复杂。只有当模型结构与检查点的结构完全一致时，状态字典才能成功加载。如果我在创建模型时使用了不同的结构，便无法顺利加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tMissing key(s) in state_dict: \"hidden_layers.2.weight\", \"hidden_layers.2.bias\". \n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([500, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([500]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([100, 500]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b678ac1471ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tMissing key(s) in state_dict: \"hidden_layers.2.weight\", \"hidden_layers.2.bias\". \n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([500, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([500]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([100, 500]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([100])."
     ]
    }
   ],
   "source": [
    "# Try this 这里建立了3层隐藏层 分别有400 200 100个单元\n",
    "net = Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------------------------------\n",
    "\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "\n",
    "~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\n",
    "    481                 try:\n",
    "--> 482                     own_state[name].copy_(param)\n",
    "    483                 except Exception:\n",
    "\n",
    "\n",
    "RuntimeError: inconsistent tensor size, expected tensor [400 x 784] and src [500 x 784] to have the same number of elements, but got 313600 and 392000 elements respectively at /Users/soumith/minicondabuild3/conda-bld/pytorch_1512381214802/work/torch/lib/TH/generic/THTensorCopy.c:86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "\n",
    "<ipython-input-18-74e14cc8e983> in <module>()\n",
    "      2 net = Network(784, 10, [400, 200, 100])\n",
    "      3 # This will throw an error because the tensor sizes are wrong!\n",
    "----> 4 net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\n",
    "    485                                        'whose dimensions in the model are {} and '\n",
    "    486                                        'whose dimensions in the checkpoint are {}.'\n",
    "--> 487                                        .format(name, own_state[name].size(), param.size()))\n",
    "    488             elif strict:\n",
    "    489                 raise KeyError('unexpected key \"{}\" in state_dict'\n",
    "\n",
    "\n",
    "RuntimeError: While copying the parameter named hidden_layers.0.weight, whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([500, 784]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着我们需要重建一个与训练时完全相同的模型。有关模型结构的信息需要与状态字典一起存储在检查点中。为了做到这一点，你需要构建一个字典，字典中包含重建模型的全部信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，检查点中包含了重建训练模型所需的全部信息。你可以随意将它编写为函数。相似地，我们也可以编写一个函数来加载检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = Network(checkpoint['input_size'],\n",
    "                    checkpoint['output_size'],\n",
    "                    checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
